{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- I used the following to help learn how to fine tune Llama2\n",
    "  - https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
    "  - https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/\n",
    "  - https://huggingface.co/docs/trl/main/en/sft_trainer\n",
    "  - https://huggingface.co/blog/llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of Virginia Tech?</td>\n",
       "      <td>Virginia Polytechnic Institute and State Unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where is the main campus of Virginia Tech loca...</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many students does Virginia Tech have?</td>\n",
       "      <td>37,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the classification of Virginia Tech am...</td>\n",
       "      <td>R1: Doctoral Universities - Very high research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the athletic teams of Virginia Tech c...</td>\n",
       "      <td>Virginia Tech Hokies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0            What is the full name of Virginia Tech?   \n",
       "1  Where is the main campus of Virginia Tech loca...   \n",
       "2         How many students does Virginia Tech have?   \n",
       "3  What is the classification of Virginia Tech am...   \n",
       "4  What are the athletic teams of Virginia Tech c...   \n",
       "\n",
       "                                              answer  \n",
       "0  Virginia Polytechnic Institute and State Unive...  \n",
       "1                               Blacksburg, Virginia  \n",
       "2                                             37,000  \n",
       "3  R1: Doctoral Universities - Very high research...  \n",
       "4                               Virginia Tech Hokies  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('data/data_filtered.csv').drop(columns=['Unnamed: 0', 'id', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the template string for fine-tuning\n",
    "template = '''<s>[INST] <<SYS>>\n",
    "You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\n",
    "<</SYS>>\n",
    "\n",
    "{} [/INST] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "1  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "2  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "3  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "4  <s>[INST] <<SYS>>\\nYou are an expert on Virgin..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt'] = df['question'].apply(lambda x: template.format(x))\n",
    "df = df.rename(columns={'answer': 'response'})\n",
    "df['response'] = df['response'] + ' </s>'\n",
    "df_train = df[['prompt', 'response']]\n",
    "df_train['text'] = df_train['prompt'] + df_train['response']\n",
    "df_train = df_train.drop(columns=['prompt', 'response'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\\n<</SYS>>\\n\\nWhat is the full name of Virginia Tech? [/INST] Virginia Polytechnic Institute and State University (VPI) </s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('data/llama2_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('data', data_files='llama2_data.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\\n<</SYS>>\\n\\nWhat is the full name of Virginia Tech? [/INST] Virginia Polytechnic Institute and State University (VPI) </s>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4634acf04c6c4a009cf1d5e887af609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-7b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4239733989429f82581966e9ac960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1228, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.6047, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.73, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.4708, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.6721, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.4559, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.6879, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.3956, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.6422, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.4291, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.6908, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.4114, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 0.5467, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
      "{'loss': 0.384, 'learning_rate': 0.0002, 'epoch': 1.17}\n",
      "{'loss': 0.4959, 'learning_rate': 0.0002, 'epoch': 1.25}\n",
      "{'loss': 0.3535, 'learning_rate': 0.0002, 'epoch': 1.34}\n",
      "{'loss': 0.4701, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
      "{'loss': 0.3636, 'learning_rate': 0.0002, 'epoch': 1.51}\n",
      "{'loss': 0.4983, 'learning_rate': 0.0002, 'epoch': 1.59}\n",
      "{'loss': 0.3457, 'learning_rate': 0.0002, 'epoch': 1.67}\n",
      "{'loss': 0.4693, 'learning_rate': 0.0002, 'epoch': 1.76}\n",
      "{'loss': 0.3596, 'learning_rate': 0.0002, 'epoch': 1.84}\n",
      "{'loss': 0.531, 'learning_rate': 0.0002, 'epoch': 1.92}\n",
      "{'loss': 0.3537, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
      "{'loss': 0.3777, 'learning_rate': 0.0002, 'epoch': 2.09}\n",
      "{'loss': 0.2743, 'learning_rate': 0.0002, 'epoch': 2.17}\n",
      "{'loss': 0.3644, 'learning_rate': 0.0002, 'epoch': 2.26}\n",
      "{'loss': 0.2981, 'learning_rate': 0.0002, 'epoch': 2.34}\n",
      "{'loss': 0.374, 'learning_rate': 0.0002, 'epoch': 2.42}\n",
      "{'loss': 0.3032, 'learning_rate': 0.0002, 'epoch': 2.51}\n",
      "{'loss': 0.3833, 'learning_rate': 0.0002, 'epoch': 2.59}\n",
      "{'loss': 0.3023, 'learning_rate': 0.0002, 'epoch': 2.68}\n",
      "{'loss': 0.3286, 'learning_rate': 0.0002, 'epoch': 2.76}\n",
      "{'loss': 0.2804, 'learning_rate': 0.0002, 'epoch': 2.84}\n",
      "{'loss': 0.3661, 'learning_rate': 0.0002, 'epoch': 2.93}\n",
      "{'loss': 0.2809, 'learning_rate': 0.0002, 'epoch': 3.01}\n",
      "{'loss': 0.2697, 'learning_rate': 0.0002, 'epoch': 3.09}\n",
      "{'loss': 0.2133, 'learning_rate': 0.0002, 'epoch': 3.18}\n",
      "{'loss': 0.282, 'learning_rate': 0.0002, 'epoch': 3.26}\n",
      "{'loss': 0.2266, 'learning_rate': 0.0002, 'epoch': 3.34}\n",
      "{'loss': 0.2875, 'learning_rate': 0.0002, 'epoch': 3.43}\n",
      "{'loss': 0.2442, 'learning_rate': 0.0002, 'epoch': 3.51}\n",
      "{'loss': 0.2532, 'learning_rate': 0.0002, 'epoch': 3.6}\n",
      "{'loss': 0.2508, 'learning_rate': 0.0002, 'epoch': 3.68}\n",
      "{'loss': 0.284, 'learning_rate': 0.0002, 'epoch': 3.76}\n",
      "{'loss': 0.2277, 'learning_rate': 0.0002, 'epoch': 3.85}\n",
      "{'loss': 0.2867, 'learning_rate': 0.0002, 'epoch': 3.93}\n",
      "{'loss': 0.235, 'learning_rate': 0.0002, 'epoch': 4.01}\n",
      "{'loss': 0.1959, 'learning_rate': 0.0002, 'epoch': 4.1}\n",
      "{'loss': 0.1917, 'learning_rate': 0.0002, 'epoch': 4.18}\n",
      "{'loss': 0.21, 'learning_rate': 0.0002, 'epoch': 4.26}\n",
      "{'loss': 0.1962, 'learning_rate': 0.0002, 'epoch': 4.35}\n",
      "{'loss': 0.2032, 'learning_rate': 0.0002, 'epoch': 4.43}\n",
      "{'loss': 0.201, 'learning_rate': 0.0002, 'epoch': 4.52}\n",
      "{'loss': 0.2279, 'learning_rate': 0.0002, 'epoch': 4.6}\n",
      "{'loss': 0.1931, 'learning_rate': 0.0002, 'epoch': 4.68}\n",
      "{'loss': 0.2093, 'learning_rate': 0.0002, 'epoch': 4.77}\n",
      "{'loss': 0.217, 'learning_rate': 0.0002, 'epoch': 4.85}\n",
      "{'loss': 0.224, 'learning_rate': 0.0002, 'epoch': 4.93}\n",
      "{'train_runtime': 840.1545, 'train_samples_per_second': 7.112, 'train_steps_per_second': 1.779, 'train_loss': 0.38290354655339165, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(fine_tuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-7b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05ddcd83afb404cab434c20fd14469f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great question! The admission rate for Virginia Tech, also known as Virginia Polytechnic Institute and State University, varies depending on the program or major to which you are applying.\n",
      "\n",
      "For the class of 2024, Virginia Tech accepted 53.6% of applicants. However, this number can vary from year to year, so it's important to check the most recent admission statistics on the university's website.\n",
      "\n",
      "Here are the admission rates for the past few years:\n",
      "\n",
      "* Class of 2024: 53.6%\n",
      "* Class\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the admission rate of Virginia Tech?\"\n",
    "text_gen = pipeline('text-generation', model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "print(output[0]['generated_text'].split('[/INST]')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b93498ff3474d418073767a224aa78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "finetuned_model.config.use_cache = False\n",
    "finetuned_model.config.pretraining_tp = 1\n",
    "finetuned_model.load_adapter(fine_tuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65.8% in 2019 and 66.8% in 2018. \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the admission rate of Virginia Tech?\"\n",
    "text_gen = pipeline('text-generation', model=finetuned_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "print(output[0]['generated_text'].split('[/INST]')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>7b_base_answer</th>\n",
       "      <th>7b_ft_answer</th>\n",
       "      <th>13b_base_answer</th>\n",
       "      <th>13b_ft_answer</th>\n",
       "      <th>verbatim?</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is a former NASA engineer and a Virginia T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As an expert on Virginia Tech, I can tell yo...</td>\n",
       "      <td>1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...</td>\n",
       "      <td>True</td>\n",
       "      <td>Homer Hickham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Upper Quad at Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ah, you must be referring to the iconic Uppe...</td>\n",
       "      <td>1876 Commencement Quadrangle 2.168 km2 of the...</td>\n",
       "      <td>True</td>\n",
       "      <td>an area on the north of the Drillfield that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where is the Virginia Tech campus located?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello! Virginia Tech's main campus is locate...</td>\n",
       "      <td>2601 Wright St. SE, Blacksburg, VA 24061 2601...</td>\n",
       "      <td>True</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the current president of Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The current president of Virginia Tech is Dr...</td>\n",
       "      <td>Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...</td>\n",
       "      <td>True</td>\n",
       "      <td>Timothy Sands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Drillfield?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ah, you must be referring to the Drillfield,...</td>\n",
       "      <td>526 acres of open field 1400 feet from end to...</td>\n",
       "      <td>True</td>\n",
       "      <td>a large oval field in the center of the Blacks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  7b_base_answer  \\\n",
       "0  Who is a former NASA engineer and a Virginia T...             NaN   \n",
       "1           What is the Upper Quad at Virginia Tech?             NaN   \n",
       "2         Where is the Virginia Tech campus located?             NaN   \n",
       "3     Who is the current president of Virginia Tech?             NaN   \n",
       "4                            What is the Drillfield?             NaN   \n",
       "\n",
       "   7b_ft_answer                                    13b_base_answer  \\\n",
       "0           NaN    As an expert on Virginia Tech, I can tell yo...   \n",
       "1           NaN    Ah, you must be referring to the iconic Uppe...   \n",
       "2           NaN    Hello! Virginia Tech's main campus is locate...   \n",
       "3           NaN    The current president of Virginia Tech is Dr...   \n",
       "4           NaN    Ah, you must be referring to the Drillfield,...   \n",
       "\n",
       "                                       13b_ft_answer  verbatim?  \\\n",
       "0   1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...       True   \n",
       "1   1876 Commencement Quadrangle 2.168 km2 of the...       True   \n",
       "2   2601 Wright St. SE, Blacksburg, VA 24061 2601...       True   \n",
       "3    Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...       True   \n",
       "4   526 acres of open field 1400 feet from end to...       True   \n",
       "\n",
       "                                               truth  \n",
       "0                                      Homer Hickham  \n",
       "1  an area on the north of the Drillfield that is...  \n",
       "2                               Blacksburg, Virginia  \n",
       "3                                      Timothy Sands  \n",
       "4  a large oval field in the center of the Blacks...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e0a67f568348a3b8f7928e5f8af51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae11e32f1c344b188b7f829b1013f9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-7b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "finetuned_model.config.use_cache = False\n",
    "finetuned_model.config.pretraining_tp = 1\n",
    "finetuned_model.load_adapter(fine_tuned_model_name)\n",
    "\n",
    "text_gen_base = pipeline('text-generation', model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "text_gen_finetuned = pipeline('text-generation', model=finetuned_model, tokenizer=llama_tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is a former NASA engineer and a Virginia Tech alumni?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2377065/3821245909.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '  Great question! I'm happy to help.\n",
      "\n",
      "After conducting a quick search, I found that there are several former NASA engineers who are also Virginia Tech alumni. However, I couldn't find a specific individual who fits your criteria.\n",
      "\n",
      "Virginia Tech has a strong reputation for producing talented engineers and scientists, and many of its graduates have gone on to work at NASA and other prestigious institutions. Some notable Virginia Tech alumni who have worked at NASA include:\n",
      "\n",
      "1. Dr. Mae Jemison - Dr.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[idx, '7b_base_answer'] = output_base[0]['generated_text'].split('[/INST]')[1]\n",
      "/tmp/ipykernel_2377065/3821245909.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value ' 2017 National Book Award finalist for his memoir, \"Warmer\" (2016) and \"The Warmth of Other Suns\" (2010). ' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[idx, '7b_ft_answer'] = output_finetuned[0]['generated_text'].split('[/INST]')[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Upper Quad at Virginia Tech?\n",
      "Where is the Virginia Tech campus located?\n",
      "Who is the current president of Virginia Tech?\n",
      "What is the Drillfield?\n",
      "What waterway runs beneath the Drillfield?\n",
      "How many alumni does Virginia Tech have internationally and from all 50 states?\n",
      "How many generals and admirals has Virginia Tech produced?\n",
      "How many Virginia Tech alumni have been awarded the Medal of Honor?\n",
      "When was the word 'Hokie' first used?\n",
      "Who came up with the spirit cheer 'Old Hokie'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the original spirit cheer?\n",
      "What was the original nickname for Hokies?\n",
      "When do students who dress as the HokieBird reveal their secret identity?\n",
      "What is the requirement for new central campus buildings at Virginia Tech?\n",
      "What do The Pylons represent from left to right?\n",
      "Who transformed VPI into a major research university?\n",
      "What did Herbert Thomas do to receive the Medal of Honor?\n",
      "Who was the first to register at Virginia Tech?\n",
      "When did the display of the Confederate flag at Virginia Tech end?\n",
      "What is the GPA requirement for students in the Honors College at Virginia Tech?\n",
      "What is the average SAT score for admitted students at Virginia Tech?\n",
      "Which Virginia Tech golfer won three PGA Tour wins?\n",
      "What is the chorus of the Alma Mater?\n",
      "Who wrote the lyrics for the Alma Mater?\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    query = row['question']\n",
    "    print(query)\n",
    "    output_base = text_gen_base(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "    output_finetuned = text_gen_finetuned(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "    df.loc[idx, '7b_base_answer'] = output_base[0]['generated_text'].split('[/INST]')[1]\n",
    "    df.loc[idx, '7b_ft_answer'] = output_finetuned[0]['generated_text'].split('[/INST]')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>7b_base_answer</th>\n",
       "      <th>7b_ft_answer</th>\n",
       "      <th>13b_base_answer</th>\n",
       "      <th>13b_ft_answer</th>\n",
       "      <th>verbatim?</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is a former NASA engineer and a Virginia T...</td>\n",
       "      <td>Great question! I'm happy to help.\\n\\nAfter ...</td>\n",
       "      <td>2017 National Book Award finalist for his mem...</td>\n",
       "      <td>As an expert on Virginia Tech, I can tell yo...</td>\n",
       "      <td>1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...</td>\n",
       "      <td>True</td>\n",
       "      <td>Homer Hickham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Upper Quad at Virginia Tech?</td>\n",
       "      <td>Great question! The Upper Quad at Virginia T...</td>\n",
       "      <td>1965 Veterans Memorial Building and the Corps...</td>\n",
       "      <td>Ah, you must be referring to the iconic Uppe...</td>\n",
       "      <td>1876 Commencement Quadrangle 2.168 km2 of the...</td>\n",
       "      <td>True</td>\n",
       "      <td>an area on the north of the Drillfield that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where is the Virginia Tech campus located?</td>\n",
       "      <td>Great question! Virginia Tech is located in ...</td>\n",
       "      <td>15 miles south of Roanoke and 75 miles southw...</td>\n",
       "      <td>Hello! Virginia Tech's main campus is locate...</td>\n",
       "      <td>2601 Wright St. SE, Blacksburg, VA 24061 2601...</td>\n",
       "      <td>True</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the current president of Virginia Tech?</td>\n",
       "      <td>I'm happy to help! The current president of ...</td>\n",
       "      <td>Timothy Sands. http://www.vtnews.org/stories/...</td>\n",
       "      <td>The current president of Virginia Tech is Dr...</td>\n",
       "      <td>Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...</td>\n",
       "      <td>True</td>\n",
       "      <td>Timothy Sands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Drillfield?</td>\n",
       "      <td>Ah, a question about my alma mater! The Dril...</td>\n",
       "      <td>a large oval field in the center of the Blac...</td>\n",
       "      <td>Ah, you must be referring to the Drillfield,...</td>\n",
       "      <td>526 acres of open field 1400 feet from end to...</td>\n",
       "      <td>True</td>\n",
       "      <td>a large oval field in the center of the Blacks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Who is a former NASA engineer and a Virginia T...   \n",
       "1           What is the Upper Quad at Virginia Tech?   \n",
       "2         Where is the Virginia Tech campus located?   \n",
       "3     Who is the current president of Virginia Tech?   \n",
       "4                            What is the Drillfield?   \n",
       "\n",
       "                                      7b_base_answer  \\\n",
       "0    Great question! I'm happy to help.\\n\\nAfter ...   \n",
       "1    Great question! The Upper Quad at Virginia T...   \n",
       "2    Great question! Virginia Tech is located in ...   \n",
       "3    I'm happy to help! The current president of ...   \n",
       "4    Ah, a question about my alma mater! The Dril...   \n",
       "\n",
       "                                        7b_ft_answer  \\\n",
       "0   2017 National Book Award finalist for his mem...   \n",
       "1   1965 Veterans Memorial Building and the Corps...   \n",
       "2   15 miles south of Roanoke and 75 miles southw...   \n",
       "3   Timothy Sands. http://www.vtnews.org/stories/...   \n",
       "4    a large oval field in the center of the Blac...   \n",
       "\n",
       "                                     13b_base_answer  \\\n",
       "0    As an expert on Virginia Tech, I can tell yo...   \n",
       "1    Ah, you must be referring to the iconic Uppe...   \n",
       "2    Hello! Virginia Tech's main campus is locate...   \n",
       "3    The current president of Virginia Tech is Dr...   \n",
       "4    Ah, you must be referring to the Drillfield,...   \n",
       "\n",
       "                                       13b_ft_answer  verbatim?  \\\n",
       "0   1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...       True   \n",
       "1   1876 Commencement Quadrangle 2.168 km2 of the...       True   \n",
       "2   2601 Wright St. SE, Blacksburg, VA 24061 2601...       True   \n",
       "3    Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...       True   \n",
       "4   526 acres of open field 1400 feet from end to...       True   \n",
       "\n",
       "                                               truth  \n",
       "0                                      Homer Hickham  \n",
       "1  an area on the north of the Drillfield that is...  \n",
       "2                               Blacksburg, Virginia  \n",
       "3                                      Timothy Sands  \n",
       "4  a large oval field in the center of the Blacks...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5624",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
