{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- I used the following to help learn how to fine tune Llama2\n",
    "  - https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n",
    "  - https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/\n",
    "  - https://huggingface.co/docs/trl/main/en/sft_trainer\n",
    "  - https://huggingface.co/blog/llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of Virginia Tech?</td>\n",
       "      <td>Virginia Polytechnic Institute and State Unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where is the main campus of Virginia Tech loca...</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many students does Virginia Tech have?</td>\n",
       "      <td>37,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the classification of Virginia Tech am...</td>\n",
       "      <td>R1: Doctoral Universities - Very high research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the athletic teams of Virginia Tech c...</td>\n",
       "      <td>Virginia Tech Hokies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0            What is the full name of Virginia Tech?   \n",
       "1  Where is the main campus of Virginia Tech loca...   \n",
       "2         How many students does Virginia Tech have?   \n",
       "3  What is the classification of Virginia Tech am...   \n",
       "4  What are the athletic teams of Virginia Tech c...   \n",
       "\n",
       "                                              answer  \n",
       "0  Virginia Polytechnic Institute and State Unive...  \n",
       "1                               Blacksburg, Virginia  \n",
       "2                                             37,000  \n",
       "3  R1: Doctoral Universities - Very high research...  \n",
       "4                               Virginia Tech Hokies  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('data/data_filtered.csv').drop(columns=['Unnamed: 0', 'id', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the template string for fine-tuning\n",
    "template = '''<s>[INST] <<SYS>>\n",
    "You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\n",
    "<</SYS>>\n",
    "\n",
    "{} [/INST] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are an expert on Virgin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "1  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "2  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "3  <s>[INST] <<SYS>>\\nYou are an expert on Virgin...\n",
       "4  <s>[INST] <<SYS>>\\nYou are an expert on Virgin..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt'] = df['question'].apply(lambda x: template.format(x))\n",
    "df = df.rename(columns={'answer': 'response'})\n",
    "df['response'] = df['response'] + ' </s>'\n",
    "df_train = df[['prompt', 'response']]\n",
    "df_train['text'] = df_train['prompt'] + df_train['response']\n",
    "df_train = df_train.drop(columns=['prompt', 'response'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\\n<</SYS>>\\n\\nWhat is the full name of Virginia Tech? [/INST] Virginia Polytechnic Institute and State University (VPI) </s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('data/llama2_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('data', data_files='llama2_data.csv', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\".\\n<</SYS>>\\n\\nWhat is the full name of Virginia Tech? [/INST] Virginia Polytechnic Institute and State University (VPI) </s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c108287ecc0649de8fef04f82d558af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-13b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlancewilhelm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lance/Projects/cs5624/wandb/run-20231121_163112-p8pcyp0h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lancewilhelm/huggingface/runs/p8pcyp0h' target=\"_blank\">worthy-silence-13</a></strong> to <a href='https://wandb.ai/lancewilhelm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lancewilhelm/huggingface' target=\"_blank\">https://wandb.ai/lancewilhelm/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lancewilhelm/huggingface/runs/p8pcyp0h' target=\"_blank\">https://wandb.ai/lancewilhelm/huggingface/runs/p8pcyp0h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186e9013957d49c8ad53d33c4739bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.039, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.6631, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 0.7868, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.5177, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.6529, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.4213, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.6584, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.4581, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.6398, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.4434, 'learning_rate': 0.0002, 'epoch': 0.42}\n",
      "{'loss': 0.6484, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.3801, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.6011, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.4043, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.5765, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
      "{'loss': 0.3849, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.5897, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.4194, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.5997, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.3639, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.5393, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 0.3735, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.5342, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.3312, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'loss': 0.4114, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
      "{'loss': 0.3219, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
      "{'loss': 0.4128, 'learning_rate': 0.0002, 'epoch': 1.13}\n",
      "{'loss': 0.3401, 'learning_rate': 0.0002, 'epoch': 1.17}\n",
      "{'loss': 0.3991, 'learning_rate': 0.0002, 'epoch': 1.21}\n",
      "{'loss': 0.2984, 'learning_rate': 0.0002, 'epoch': 1.25}\n",
      "{'loss': 0.3861, 'learning_rate': 0.0002, 'epoch': 1.3}\n",
      "{'loss': 0.3118, 'learning_rate': 0.0002, 'epoch': 1.34}\n",
      "{'loss': 0.3972, 'learning_rate': 0.0002, 'epoch': 1.38}\n",
      "{'loss': 0.316, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
      "{'loss': 0.4248, 'learning_rate': 0.0002, 'epoch': 1.46}\n",
      "{'loss': 0.3461, 'learning_rate': 0.0002, 'epoch': 1.51}\n",
      "{'loss': 0.4439, 'learning_rate': 0.0002, 'epoch': 1.55}\n",
      "{'loss': 0.3283, 'learning_rate': 0.0002, 'epoch': 1.59}\n",
      "{'loss': 0.3619, 'learning_rate': 0.0002, 'epoch': 1.63}\n",
      "{'loss': 0.298, 'learning_rate': 0.0002, 'epoch': 1.67}\n",
      "{'loss': 0.3979, 'learning_rate': 0.0002, 'epoch': 1.71}\n",
      "{'loss': 0.3314, 'learning_rate': 0.0002, 'epoch': 1.76}\n",
      "{'loss': 0.4287, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
      "{'loss': 0.3262, 'learning_rate': 0.0002, 'epoch': 1.84}\n",
      "{'loss': 0.3898, 'learning_rate': 0.0002, 'epoch': 1.88}\n",
      "{'loss': 0.2921, 'learning_rate': 0.0002, 'epoch': 1.92}\n",
      "{'loss': 0.4571, 'learning_rate': 0.0002, 'epoch': 1.96}\n",
      "{'loss': 0.3495, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
      "{'loss': 0.2896, 'learning_rate': 0.0002, 'epoch': 2.05}\n",
      "{'loss': 0.2386, 'learning_rate': 0.0002, 'epoch': 2.09}\n",
      "{'loss': 0.2942, 'learning_rate': 0.0002, 'epoch': 2.13}\n",
      "{'loss': 0.2381, 'learning_rate': 0.0002, 'epoch': 2.17}\n",
      "{'loss': 0.2358, 'learning_rate': 0.0002, 'epoch': 2.22}\n",
      "{'loss': 0.2594, 'learning_rate': 0.0002, 'epoch': 2.26}\n",
      "{'loss': 0.2828, 'learning_rate': 0.0002, 'epoch': 2.3}\n",
      "{'loss': 0.2367, 'learning_rate': 0.0002, 'epoch': 2.34}\n",
      "{'loss': 0.2532, 'learning_rate': 0.0002, 'epoch': 2.38}\n",
      "{'loss': 0.2627, 'learning_rate': 0.0002, 'epoch': 2.42}\n",
      "{'loss': 0.2738, 'learning_rate': 0.0002, 'epoch': 2.47}\n",
      "{'loss': 0.2499, 'learning_rate': 0.0002, 'epoch': 2.51}\n",
      "{'loss': 0.3024, 'learning_rate': 0.0002, 'epoch': 2.55}\n",
      "{'loss': 0.245, 'learning_rate': 0.0002, 'epoch': 2.59}\n",
      "{'loss': 0.2862, 'learning_rate': 0.0002, 'epoch': 2.63}\n",
      "{'loss': 0.2553, 'learning_rate': 0.0002, 'epoch': 2.68}\n",
      "{'loss': 0.2366, 'learning_rate': 0.0002, 'epoch': 2.72}\n",
      "{'loss': 0.2583, 'learning_rate': 0.0002, 'epoch': 2.76}\n",
      "{'loss': 0.2643, 'learning_rate': 0.0002, 'epoch': 2.8}\n",
      "{'loss': 0.2877, 'learning_rate': 0.0002, 'epoch': 2.84}\n",
      "{'loss': 0.3314, 'learning_rate': 0.0002, 'epoch': 2.88}\n",
      "{'loss': 0.239, 'learning_rate': 0.0002, 'epoch': 2.93}\n",
      "{'loss': 0.2957, 'learning_rate': 0.0002, 'epoch': 2.97}\n",
      "{'loss': 0.2409, 'learning_rate': 0.0002, 'epoch': 3.01}\n",
      "{'loss': 0.198, 'learning_rate': 0.0002, 'epoch': 3.05}\n",
      "{'loss': 0.1813, 'learning_rate': 0.0002, 'epoch': 3.09}\n",
      "{'loss': 0.2014, 'learning_rate': 0.0002, 'epoch': 3.14}\n",
      "{'loss': 0.1865, 'learning_rate': 0.0002, 'epoch': 3.18}\n",
      "{'loss': 0.1891, 'learning_rate': 0.0002, 'epoch': 3.22}\n",
      "{'loss': 0.196, 'learning_rate': 0.0002, 'epoch': 3.26}\n",
      "{'loss': 0.1952, 'learning_rate': 0.0002, 'epoch': 3.3}\n",
      "{'loss': 0.1911, 'learning_rate': 0.0002, 'epoch': 3.34}\n",
      "{'loss': 0.2227, 'learning_rate': 0.0002, 'epoch': 3.39}\n",
      "{'loss': 0.2128, 'learning_rate': 0.0002, 'epoch': 3.43}\n",
      "{'loss': 0.1899, 'learning_rate': 0.0002, 'epoch': 3.47}\n",
      "{'loss': 0.1929, 'learning_rate': 0.0002, 'epoch': 3.51}\n",
      "{'loss': 0.21, 'learning_rate': 0.0002, 'epoch': 3.55}\n",
      "{'loss': 0.2148, 'learning_rate': 0.0002, 'epoch': 3.6}\n",
      "{'loss': 0.2149, 'learning_rate': 0.0002, 'epoch': 3.64}\n",
      "{'loss': 0.1935, 'learning_rate': 0.0002, 'epoch': 3.68}\n",
      "{'loss': 0.2179, 'learning_rate': 0.0002, 'epoch': 3.72}\n",
      "{'loss': 0.1801, 'learning_rate': 0.0002, 'epoch': 3.76}\n",
      "{'loss': 0.187, 'learning_rate': 0.0002, 'epoch': 3.8}\n",
      "{'loss': 0.2239, 'learning_rate': 0.0002, 'epoch': 3.85}\n",
      "{'loss': 0.2249, 'learning_rate': 0.0002, 'epoch': 3.89}\n",
      "{'loss': 0.1978, 'learning_rate': 0.0002, 'epoch': 3.93}\n",
      "{'loss': 0.1941, 'learning_rate': 0.0002, 'epoch': 3.97}\n",
      "{'loss': 0.176, 'learning_rate': 0.0002, 'epoch': 4.01}\n",
      "{'loss': 0.1601, 'learning_rate': 0.0002, 'epoch': 4.06}\n",
      "{'loss': 0.1562, 'learning_rate': 0.0002, 'epoch': 4.1}\n",
      "{'loss': 0.1586, 'learning_rate': 0.0002, 'epoch': 4.14}\n",
      "{'loss': 0.1445, 'learning_rate': 0.0002, 'epoch': 4.18}\n",
      "{'loss': 0.1601, 'learning_rate': 0.0002, 'epoch': 4.22}\n",
      "{'loss': 0.1741, 'learning_rate': 0.0002, 'epoch': 4.26}\n",
      "{'loss': 0.1597, 'learning_rate': 0.0002, 'epoch': 4.31}\n",
      "{'loss': 0.174, 'learning_rate': 0.0002, 'epoch': 4.35}\n",
      "{'loss': 0.1586, 'learning_rate': 0.0002, 'epoch': 4.39}\n",
      "{'loss': 0.1534, 'learning_rate': 0.0002, 'epoch': 4.43}\n",
      "{'loss': 0.1634, 'learning_rate': 0.0002, 'epoch': 4.47}\n",
      "{'loss': 0.1612, 'learning_rate': 0.0002, 'epoch': 4.52}\n",
      "{'loss': 0.1564, 'learning_rate': 0.0002, 'epoch': 4.56}\n",
      "{'loss': 0.156, 'learning_rate': 0.0002, 'epoch': 4.6}\n",
      "{'loss': 0.1667, 'learning_rate': 0.0002, 'epoch': 4.64}\n",
      "{'loss': 0.1718, 'learning_rate': 0.0002, 'epoch': 4.68}\n",
      "{'loss': 0.1652, 'learning_rate': 0.0002, 'epoch': 4.72}\n",
      "{'loss': 0.1758, 'learning_rate': 0.0002, 'epoch': 4.77}\n",
      "{'loss': 0.1657, 'learning_rate': 0.0002, 'epoch': 4.81}\n",
      "{'loss': 0.173, 'learning_rate': 0.0002, 'epoch': 4.85}\n",
      "{'loss': 0.173, 'learning_rate': 0.0002, 'epoch': 4.89}\n",
      "{'loss': 0.1667, 'learning_rate': 0.0002, 'epoch': 4.93}\n",
      "{'loss': 0.173, 'learning_rate': 0.0002, 'epoch': 4.97}\n",
      "{'train_runtime': 2274.1179, 'train_samples_per_second': 2.627, 'train_steps_per_second': 1.315, 'train_loss': 0.31618335741419457, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "fine_tuning.train()\n",
    "\n",
    "# Save Model\n",
    "fine_tuning.model.save_pretrained(fine_tuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-13b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df16f092951a41bc80a261532de3c871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As an expert on Virginia Tech, I can tell you that one notable former NASA engineer and Virginia Tech alumnus is Dr. Wernher von Braun. Dr. von Braun earned his master's degree in aeronautical engineering from Virginia Tech in 1935, and later went on to become a leading figure in the development of rocket technology at NASA. He played a key role in the development of the Saturn V rocket that took astronauts to the moon during the Apollo program, and was also involved in the development of other advanced spacecraft and technologies. Dr\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Upper Quad at Virginia Tech?\"\n",
    "text_gen = pipeline('text-generation', model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "print(output[0]['generated_text'].split('[/INST]')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97c6a440c194adb9c7c6e6ba27c910a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "finetuned_model.config.use_cache = False\n",
    "finetuned_model.config.pretraining_tp = 1\n",
    "finetuned_model.load_adapter(fine_tuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1964 Homer Hadley 'Sonny' Hickam 2007 Virginia Tech Astronaut Scholars 2017 Virginia Tech Astronaut Scholars 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Upper Quad at Virginia Tech?\"\n",
    "text_gen = pipeline('text-generation', model=finetuned_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "print(output[0]['generated_text'].split('[/INST]')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>7b_base_answer</th>\n",
       "      <th>7b_ft_answer</th>\n",
       "      <th>13b_base_answer</th>\n",
       "      <th>13b_ft_answer</th>\n",
       "      <th>verbatim?</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is a former NASA engineer and a Virginia T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Homer Hickham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Upper Quad at Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>an area on the north of the Drillfield that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where is the Virginia Tech campus located?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the current president of Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Timothy Sands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Drillfield?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>a large oval field in the center of the Blacks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  7b_base_answer  \\\n",
       "0  Who is a former NASA engineer and a Virginia T...             NaN   \n",
       "1           What is the Upper Quad at Virginia Tech?             NaN   \n",
       "2         Where is the Virginia Tech campus located?             NaN   \n",
       "3     Who is the current president of Virginia Tech?             NaN   \n",
       "4                            What is the Drillfield?             NaN   \n",
       "\n",
       "   7b_ft_answer  13b_base_answer  13b_ft_answer  verbatim?  \\\n",
       "0           NaN              NaN            NaN       True   \n",
       "1           NaN              NaN            NaN       True   \n",
       "2           NaN              NaN            NaN       True   \n",
       "3           NaN              NaN            NaN       True   \n",
       "4           NaN              NaN            NaN       True   \n",
       "\n",
       "                                               truth  \n",
       "0                                      Homer Hickham  \n",
       "1  an area on the north of the Drillfield that is...  \n",
       "2                               Blacksburg, Virginia  \n",
       "3                                      Timothy Sands  \n",
       "4  a large oval field in the center of the Blacks...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Names\n",
    "base_model_name = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "fine_tuned_model_name = \"llama2-13b-hokiehelper\"\n",
    "\n",
    "# Tokenizer Names\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = 'right'\n",
    "\n",
    "# Quanization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "finetuned_model.config.use_cache = False\n",
    "finetuned_model.config.pretraining_tp = 1\n",
    "finetuned_model.load_adapter(fine_tuned_model_name)\n",
    "\n",
    "text_gen_base = pipeline('text-generation', model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "text_gen_finetuned = pipeline('text-generation', model=finetuned_model, tokenizer=llama_tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is a former NASA engineer and a Virginia Tech alumni?\n",
      "What is the Upper Quad at Virginia Tech?\n",
      "Where is the Virginia Tech campus located?\n",
      "Who is the current president of Virginia Tech?\n",
      "What is the Drillfield?\n",
      "What waterway runs beneath the Drillfield?\n",
      "How many alumni does Virginia Tech have internationally and from all 50 states?\n",
      "How many generals and admirals has Virginia Tech produced?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lance/miniconda3/envs/cs5624/lib/python3.11/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many Virginia Tech alumni have been awarded the Medal of Honor?\n",
      "When was the word 'Hokie' first used?\n",
      "Who came up with the spirit cheer 'Old Hokie'?\n",
      "What was the original spirit cheer?\n",
      "What was the original nickname for Hokies?\n",
      "When do students who dress as the HokieBird reveal their secret identity?\n",
      "What is the requirement for new central campus buildings at Virginia Tech?\n",
      "What do The Pylons represent from left to right?\n",
      "Who transformed VPI into a major research university?\n",
      "What did Herbert Thomas do to receive the Medal of Honor?\n",
      "Who was the first to register at Virginia Tech?\n",
      "When did the display of the Confederate flag at Virginia Tech end?\n",
      "What is the GPA requirement for students in the Honors College at Virginia Tech?\n",
      "What is the average SAT score for admitted students at Virginia Tech?\n",
      "Which Virginia Tech golfer won three PGA Tour wins?\n",
      "What is the chorus of the Alma Mater?\n",
      "Who wrote the lyrics for the Alma Mater?\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    query = row['question']\n",
    "    print(query)\n",
    "    output_base = text_gen_base(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "    output_finetuned = text_gen_finetuned(f'<s>[INST] <<SYS>> You are an expert on Virginia Tech or Virginia Polytechnic Institute and State University. Always answer in a helpful way. If you do not know the answer, simply response with \"I do not know\". <</SYS>> {query} [/INST] ')\n",
    "    df.loc[idx, '13b_base_answer'] = output_base[0]['generated_text'].split('[/INST]')[1]\n",
    "    df.loc[idx, '13b_ft_answer'] = output_finetuned[0]['generated_text'].split('[/INST]')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>7b_base_answer</th>\n",
       "      <th>7b_ft_answer</th>\n",
       "      <th>13b_base_answer</th>\n",
       "      <th>13b_ft_answer</th>\n",
       "      <th>verbatim?</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is a former NASA engineer and a Virginia T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As an expert on Virginia Tech, I can tell yo...</td>\n",
       "      <td>1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...</td>\n",
       "      <td>True</td>\n",
       "      <td>Homer Hickham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the Upper Quad at Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ah, you must be referring to the iconic Uppe...</td>\n",
       "      <td>1876 Commencement Quadrangle 2.168 km2 of the...</td>\n",
       "      <td>True</td>\n",
       "      <td>an area on the north of the Drillfield that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where is the Virginia Tech campus located?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hello! Virginia Tech's main campus is locate...</td>\n",
       "      <td>2601 Wright St. SE, Blacksburg, VA 24061 2601...</td>\n",
       "      <td>True</td>\n",
       "      <td>Blacksburg, Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the current president of Virginia Tech?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The current president of Virginia Tech is Dr...</td>\n",
       "      <td>Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...</td>\n",
       "      <td>True</td>\n",
       "      <td>Timothy Sands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Drillfield?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ah, you must be referring to the Drillfield,...</td>\n",
       "      <td>526 acres of open field 1400 feet from end to...</td>\n",
       "      <td>True</td>\n",
       "      <td>a large oval field in the center of the Blacks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  7b_base_answer  \\\n",
       "0  Who is a former NASA engineer and a Virginia T...             NaN   \n",
       "1           What is the Upper Quad at Virginia Tech?             NaN   \n",
       "2         Where is the Virginia Tech campus located?             NaN   \n",
       "3     Who is the current president of Virginia Tech?             NaN   \n",
       "4                            What is the Drillfield?             NaN   \n",
       "\n",
       "   7b_ft_answer                                    13b_base_answer  \\\n",
       "0           NaN    As an expert on Virginia Tech, I can tell yo...   \n",
       "1           NaN    Ah, you must be referring to the iconic Uppe...   \n",
       "2           NaN    Hello! Virginia Tech's main campus is locate...   \n",
       "3           NaN    The current president of Virginia Tech is Dr...   \n",
       "4           NaN    Ah, you must be referring to the Drillfield,...   \n",
       "\n",
       "                                       13b_ft_answer  verbatim?  \\\n",
       "0   1964 Homer Hadley 'Sonny' Hickam 2007 2012 20...       True   \n",
       "1   1876 Commencement Quadrangle 2.168 km2 of the...       True   \n",
       "2   2601 Wright St. SE, Blacksburg, VA 24061 2601...       True   \n",
       "3    Timothy Sands 6 7 8 9 10 11 12-13 14 15 16 1...       True   \n",
       "4   526 acres of open field 1400 feet from end to...       True   \n",
       "\n",
       "                                               truth  \n",
       "0                                      Homer Hickham  \n",
       "1  an area on the north of the Drillfield that is...  \n",
       "2                               Blacksburg, Virginia  \n",
       "3                                      Timothy Sands  \n",
       "4  a large oval field in the center of the Blacks...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5624",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
